# Journal Recommendation System - Complete Technical Documentation

**Version**: 2.0 (Cross-Disciplinary Expansion)  
**Last Updated**: November 8, 2025  
**Status**: Production Ready ✅  
**Repository**: jourrecsystem (HarshithMandi)

---

## Table of Contents

1. [System Overview](#system-overview)
2. [Architecture](#architecture)
3. [Database Schema](#database-schema)
4. [Recommendation Algorithm](#recommendation-algorithm)
5. [Mathematical Formulations](#mathematical-formulations)
6. [API Documentation](#api-documentation)
7. [Installation & Setup](#installation--setup)
8. [Usage Examples](#usage-examples)
9. [Performance Specifications](#performance-specifications)
10. [Troubleshooting](#troubleshooting)

---

## System Overview

### Purpose
An intelligent journal recommendation system that suggests the most relevant academic journals for research paper abstracts using advanced machine learning techniques including BERT embeddings, TF-IDF analysis, and multi-component scoring.

### Key Features
- ✅ **1,776 journals** across 11 major research areas
- ✅ **Semantic understanding** using BERT transformer models
- ✅ **Keyword matching** via TF-IDF analysis
- ✅ **Multi-component scoring** (6 different signals)
- ✅ **Cross-disciplinary support** (Computer Science, Biology, Medicine, etc.)
- ✅ **RESTful API** with FastAPI
- ✅ **Interactive dashboard** with Streamlit
- ✅ **100% vector coverage** across all journals

### Research Fields Covered

| Field | Journal Count | Example Journals |
|-------|---------------|------------------|
| Computer Science | 353 | IEEE Trans., ACM Journals, Springer CS |
| Biology | 215 | Nature Biology, BMC Biology, PLOS Biology |
| Environmental Science | 155 | Climate Change, Environmental Science & Technology |
| Medicine | 228 | NEJM, The Lancet, Cancer Research |
| Physical Sciences | 135 | Physical Review, J. Chemical Physics |
| Engineering | 156 | IEEE Engineering, ASME Journals |
| Social Sciences | 205 | American J. Sociology, Psychology journals |
| Agriculture | 201 | Agronomy, Food Science, Veterinary journals |
| Earth Sciences | 128 | Geology, Oceanography, Meteorology |
| Education | Various | Education journals |
| Linguistics | Various | Applied Linguistics, Computational Linguistics |

---

## Architecture

### System Components

```
┌─────────────────────────────────────────────────────────────┐
│                         Frontend Layer                        │
├───────────────────────────────┬───────────────────────────────┤
│     Streamlit Dashboard           │    External API Clients   │
│     (dashboard.py)                │    (Any HTTP client)      │
└───────────────────────────────────┴───────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│                      API Layer (FastAPI)                      │
├───────────────────────────────────────────────────────────────┤
│  • /api/recommend             - Basic recommendations         │
│  • /api/recommend-detailed    - Full score breakdown          │
│  • /api/compare-rankings      - Algorithm comparison          │
│  • /api/analyze-text          - Abstract analysis             │
└───────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│                   Service Layer (recommender.py)             │
├───────────────────────────────────────────────────────────────┤
│  • rank_journals()            - Main ranking function         │
│  • get_ranking_comparisons()  - Multi-method comparison       │
│  • extract_keywords()         - TF-IDF keyword extraction     │
│  • calculate_*_similarity()   - Component scorers             │
└───────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│                    ML Models (Sentence Transformers)         │
├───────────────────────────────────────────────────────────────┤
│  • all-MiniLM-L6-v2           - 384-dim BERT embeddings       │
│  • TfidfVectorizer            - 1,651-dim keyword vectors     │
└───────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│                   Data Layer (SQLite + SQLAlchemy)           │
├───────────────────────────────────────────────────────────────┤
│  • journals                   - Journal metadata              │
│  • journal_profiles           - Pre-computed vectors          │
│  • query_runs                 - Query audit trail             │
│  • recommendations            - Recommendation history        │
└───────────────────────────────────────────────────────────────┘
```

### Technology Stack

**Backend**:
- Python 3.10+
- FastAPI (REST API framework)
- SQLAlchemy (ORM)
- SQLite (Database)

**Machine Learning**:
- Sentence Transformers (BERT embeddings)
- Scikit-learn (TF-IDF, similarity metrics)
- NumPy (Numerical computations)

**Frontend**:
- Streamlit (Dashboard UI)
- Plotly (Visualizations)

---

## Database Schema

### Tables Overview

```sql
-- 1. journals: Core journal metadata
CREATE TABLE journals (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    openalex_id TEXT UNIQUE NOT NULL,      -- OpenAlex identifier
    source_type TEXT NOT NULL,             -- 'openalex', 'crossref', etc.
    name TEXT NOT NULL,                    -- Official journal name
    display_name TEXT,                     -- Display name (may differ)
    issn TEXT,                             -- Print ISSN
    eissn TEXT,                            -- Electronic ISSN
    is_open_access BOOLEAN DEFAULT 0,      -- Open access status
    publisher TEXT,                        -- Publisher name or URL
    subjects TEXT,                         -- JSON array of subjects
    impact_factor REAL,                    -- Impact factor (if available)
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 2. journal_profiles: Pre-computed ML vectors
CREATE TABLE journal_profiles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    journal_id INTEGER UNIQUE NOT NULL,
    bert_vector TEXT,                      -- JSON array (384 floats)
    tfidf_vector TEXT,                     -- JSON array (1,651 floats)
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (journal_id) REFERENCES journals(id) ON DELETE CASCADE
);

-- 3. query_runs: Query audit trail
CREATE TABLE query_runs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    query_text TEXT NOT NULL,              -- User's abstract
    model_used TEXT DEFAULT 'advanced_ensemble',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 4. recommendations: Recommendation history
CREATE TABLE recommendations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    query_id INTEGER NOT NULL,
    journal_id INTEGER NOT NULL,
    similarity REAL NOT NULL,              -- Combined similarity score
    rank INTEGER NOT NULL,                 -- Rank (1, 2, 3, ...)
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (query_id) REFERENCES query_runs(id) ON DELETE CASCADE,
    FOREIGN KEY (journal_id) REFERENCES journals(id) ON DELETE CASCADE
);
```

### Vector Storage Format

Vectors are stored as JSON arrays in TEXT columns:

```python
# BERT vector (384 dimensions)
bert_vector = "[0.123, -0.456, 0.789, ..., 0.321]"  # 384 floats

# TF-IDF vector (1,651 dimensions)
tfidf_vector = "[0.0, 0.234, 0.0, ..., 0.567]"  # 1,651 floats (sparse)
```

**Storage Efficiency**:
- BERT: ~3 KB per journal
- TF-IDF: ~15 KB per journal (sparse, many zeros)
- Total: ~18 KB per journal × 1,776 = ~32 MB of vector data

### Database Statistics

```
Total Journals:        1,776
Vector Completeness:   100%
Database Size:         37.55 MB
Open Access:           439 journals (24.7%)
Closed Access:         1,337 journals (75.3%)
```

---

## Recommendation Algorithm

### High-Level Process

```
1. Input: Research paper abstract (text)
   ↓
2. Text Preprocessing:
   - Extract keywords via TF-IDF (top 10)
   - Encode abstract with BERT (384-dim)
   - Generate TF-IDF vector (1,651-dim)
   ↓
3. For each journal in database:
   - Load pre-computed vectors
   - Calculate 6 similarity scores:
     a. BERT semantic similarity
     b. TF-IDF lexical similarity
     c. Title matching score
     d. Keyword overlap score
     e. Impact factor boost
     f. Field matching boost
   - Combine scores with weighted formula
   ↓
4. Rank journals by combined score (descending)
   ↓
5. Return top K journals with detailed scores
```

### Algorithm Pseudocode

```python
def rank_journals(abstract: str, top_k: int = 10):
    # Step 1: Preprocess abstract
    keywords = extract_tfidf_keywords(abstract, top_n=10)
    vec_abstract_bert = bert_model.encode(abstract)
    vec_abstract_tfidf = tfidf_vectorizer.transform(abstract)
    
    # Step 2: Score each journal
    scores = []
    for journal in database.get_all_journals():
        # Load pre-computed vectors
        vec_journal_bert = json.loads(journal.profile.bert_vector)
        vec_journal_tfidf = json.loads(journal.profile.tfidf_vector)
        
        # Component 1: BERT similarity (50%)
        sim_bert = cosine_similarity(vec_abstract_bert, vec_journal_bert)
        
        # Component 2: TF-IDF similarity (20%)
        sim_tfidf = cosine_similarity(vec_abstract_tfidf, vec_journal_tfidf)
        
        # Component 3: Title similarity (10%)
        vec_title = bert_model.encode(journal.name)
        sim_title = cosine_similarity(vec_abstract_bert, vec_title)
        
        # Component 4: Keyword overlap (10%)
        journal_text = f"{journal.name} {journal.publisher}"
        sim_keyword = count_keyword_matches(keywords, journal_text) / len(keywords)
        
        # Component 5: Impact factor boost (5%)
        boost_impact = min(journal.impact_factor / 100, 1.0) if journal.impact_factor else 0.0
        
        # Component 6: Field matching boost (5%)
        boost_field = count_subject_matches(keywords, journal.subjects) / len(keywords)
        
        # Combined score
        score_combined = (
            0.50 * sim_bert +
            0.20 * sim_tfidf +
            0.10 * sim_title +
            0.10 * sim_keyword +
            0.05 * boost_impact +
            0.05 * boost_field
        )
        
        scores.append((journal, score_combined))
    
    # Step 3: Sort and return top K
    scores.sort(key=lambda x: x[1], reverse=True)
    return scores[:top_k]
```

---

## Mathematical Formulations

### Notation

| Symbol | Description | Dimension |
|--------|-------------|-----------|
| v_a | BERT embedding of abstract | R^384 |
| v_j | BERT embedding of journal | R^384 |
| w_a | TF-IDF vector of abstract | R^1651 |
| w_j | TF-IDF vector of journal | R^1651 |
| K_a | Set of keywords from abstract | Set of strings |
| S_j | Set of subjects of journal | Set of strings |
| IF_j | Impact factor of journal | R≥0 |
| N | Total number of journals | 1,776 |

---

### 1. Cosine Similarity (General Formula)

Used for BERT, TF-IDF, and title similarity:

**Formula**: sim_cos(a, b) = (a · b) / (||a|| ||b||)

Where:
- a · b = sum of element-wise products (dot product)
- ||a|| = sqrt(sum of squared elements) (L2 norm)
- d = dimensionality (384 for BERT, 1,651 for TF-IDF)

**Properties**:
- Range: [-1, 1] (theoretically), typically [0, 1] for our embeddings
- Value of 1: Vectors point in same direction (perfect similarity)
- Value of 0: Vectors are orthogonal (no similarity)
- Value of -1: Vectors point in opposite directions (perfect dissimilarity)

**Computational Complexity**: O(d) for each pair

---

### 2. BERT Semantic Similarity

**Formula**: s_BERT = sim_cos(v_a, v_j)

Expanded: s_BERT = sum(v_a[i] * v_j[i]) / (sqrt(sum(v_a[i]^2)) * sqrt(sum(v_j[i]^2)))

**BERT Model**: `all-MiniLM-L6-v2` (Sentence Transformers)
- **Architecture**: 6-layer MiniLM transformer
- **Training**: Distilled from larger BERT models
- **Vocabulary**: 30,522 WordPiece tokens
- **Output**: Dense 384-dimensional sentence embeddings
- **Training Data**: 1 billion sentence pairs (diverse domains)

**Encoding Process**:
1. Tokenize text
2. Pass through transformer layers
3. Apply mean pooling over token embeddings
4. Normalize to unit length

**Range**: [0, 1] (after normalization)

---

### 3. TF-IDF Lexical Similarity

**Formula**: s_TFIDF = sim_cos(w_a, w_j)

**TF-IDF Weight Calculation**:

For term t in document d:

**w(t,d) = tf(t,d) × idf(t)**

**Term Frequency (TF)**:
tf(t,d) = f(t,d) / sum(f(t',d) for all t')

Where f(t,d) = raw count of term t in document d

**Inverse Document Frequency (IDF)**:
idf(t) = log((N + 1) / (df(t) + 1)) + 1

Where:
- N = 1,776 (total journals)
- df(t) = number of journals containing term t
- +1 smoothing prevents division by zero

**Example**:
- Term "machine" appears in 200 journals
- idf_machine = log(1777/201) + 1 = log(8.84) + 1 ≈ 3.18
- Rare terms get higher IDF scores
- Common terms get lower IDF scores

**Vectorization Parameters**:
- max_features: 20,000 (maximum vocabulary size)
- stop_words: 'english' (removes common words)
- actual_features: 1,651 (from 1,776-journal corpus)

**Sparsity**: TF-IDF vectors are typically 95-99% zeros (sparse)

**Range**: [0, 1] (after L2 normalization)

---

### 4. Title Similarity

**Formula**: s_title = sim_cos(v_a^(200), v_t)

Where:
- v_a^(200) = BERT embedding of first 200 characters of abstract
- v_t = BERT embedding of journal title

**Rationale**:
- Journal titles are short (~5-10 words)
- Use truncated abstract for fair comparison
- First 200 characters usually contain key topic information
- Faster computation (shorter input)

**Implementation**:
```python
# Encode abstract once (truncated)
vec_abstract_short = bert_model.encode(abstract[:200])

# Batch encode all journal titles
journal_titles = [j.name for j in journals]
vec_titles = bert_model.encode(journal_titles)

# Compute similarities (vectorized)
sims_title = [cosine_sim(vec_abstract_short, vec_t) for vec_t in vec_titles]
```

**Range**: [0, 1]

---

### 5. Keyword Overlap Similarity

**Formula**: s_keyword = |K_a ∩ T_j| / |K_a|

Where:
- K_a = set of top 10 TF-IDF keywords from abstract
- T_j = set of words in journal text (name, publisher, etc.)
- ∩ = set intersection (case-insensitive)

**Keyword Extraction Algorithm**:
```python
def extract_keywords(abstract, top_n=10):
    # 1. Compute TF-IDF vector
    vec_tfidf = tfidf_vectorizer.transform([abstract])
    
    # 2. Get feature names (vocabulary)
    feature_names = tfidf_vectorizer.get_feature_names_out()
    
    # 3. Convert sparse vector to dense
    vec_dense = vec_tfidf.toarray()[0]
    
    # 4. Get top N indices by TF-IDF score
    top_indices = np.argsort(vec_dense)[-top_n:][::-1]
    
    # 5. Extract keywords (filter out zeros)
    keywords = [feature_names[i] for i in top_indices if vec_dense[i] > 0]
    
    return keywords
```

**Example**:
- Abstract: "Machine learning models for natural language processing..."
- Keywords: ['machine', 'learning', 'models', 'language', 'processing', ...]
- Journal: "Journal of Machine Learning Research"
- Matches: {'machine', 'learning'} → 2 matches
- Score: 2 / 10 = 0.2

**Range**: [0, 1]

---

### 6. Impact Factor Boost

**Formula**:
```
b_impact = 0                       if IF_j is null or IF_j ≤ 0
         = 1                       if IF_j ≥ 100
         = IF_j / 100              otherwise
```

Simplified: b_impact = min(max(IF_j / 100, 0), 1)

**Impact Factor** (IF):
- Measure of journal citation frequency
- Higher IF = more influential journal
- Typical ranges:
  - Top-tier: IF > 10 (e.g., Nature = 49.96)
  - Mid-tier: 2 < IF < 10
  - Lower-tier: IF < 2
  - No IF: New journals or non-indexed

**Normalization Rationale**:
- Cap at 100 to prevent extreme outliers
- Journals with IF > 100 treated equally
- Linear scaling in [0, 100] range

**Example**:
- Nature (IF = 49.96): boost = 49.96 / 100 = 0.500
- PLOS ONE (IF = 3.24): boost = 3.24 / 100 = 0.032
- New journal (IF = null): boost = 0.0

**Range**: [0, 1]

---

### 7. Field Matching Boost

**Formula**: b_field = min((1/|K_a|) * sum(1 if keyword matches any subject), 1)

Where:
- K_a = abstract keywords
- S_j = journal subject names (from OpenAlex)
- Match = keyword is substring of subject or vice versa (case-insensitive)

**Algorithm**:
```python
def calculate_field_matching(keywords, subjects):
    if not keywords or not subjects:
        return 0.0
    
    matches = 0
    keywords_lower = [k.lower() for k in keywords]
    subjects_lower = [s.lower() for s in subjects]
    
    for keyword in keywords_lower:
        for subject in subjects_lower:
            if keyword in subject or subject in keyword:
                matches += 1
                break  # Count each keyword only once
    
    return min(matches / len(keywords), 1.0)
```

**Example**:
- Keywords: ['machine', 'learning', 'neural', 'networks']
- Subjects: ['Machine Learning', 'Computer Science', 'Artificial Intelligence']
- Matches:
  - 'machine' in 'machine learning' ✓
  - 'learning' in 'machine learning' ✓
  - 'neural' → no match ✗
  - 'networks' → no match ✗
- Score: 2 / 4 = 0.5

**Range**: [0, 1]

---

### 8. Combined Scoring Formula

**Formula**: S_combined = sum(w_i * s_i) for i=1 to 6

**Component Weights**:

| i | Component | Weight w_i | Score s_i | Rationale |
|---|-----------|------------|-----------|-----------|
| 1 | BERT | 0.50 | s_BERT | Primary semantic understanding |
| 2 | TF-IDF | 0.20 | s_TFIDF | Keyword/lexical matching |
| 3 | Title | 0.10 | s_title | Journal relevance to topic |
| 4 | Keyword | 0.10 | s_keyword | Explicit term overlap |
| 5 | Impact | 0.05 | b_impact | Journal quality signal |
| 6 | Field | 0.05 | b_field | Subject area alignment |

**Expanded Formula**:
S_combined = 0.50*s_BERT + 0.20*s_TFIDF + 0.10*s_title + 0.10*s_keyword + 0.05*b_impact + 0.05*b_field

**Constraint**: sum(w_i) = 1.0 (100%)

**Weight Justification**:

1. **BERT (50%)**: Highest weight because:
   - Captures deep semantic meaning
   - Understands context and relationships
   - Most reliable for cross-domain matching
   - Trained on diverse corpora

2. **TF-IDF (20%)**: Important but secondary:
   - Captures specific terminology
   - Good for domain-specific terms
   - Complements BERT's semantic understanding
   - Less weight because it's lexical-only

3. **Title (10%)**: Moderate weight:
   - Journal titles are informative
   - But often generic (e.g., "Journal of X")
   - Useful confirmation signal

4. **Keyword (10%)**: Similar to title:
   - Explicit term matching
   - Catches domain-specific jargon
   - Overlaps with TF-IDF but more focused

5. **Impact (5%)**: Low weight:
   - Quality signal, not relevance
   - Shouldn't dominate recommendations
   - Many great journals have moderate IF

6. **Field (5%)**: Low weight:
   - Subject tags often noisy/incomplete
   - Useful tie-breaker
   - Complements other signals

**Range**: [0, 1] (theoretically), typically [0.1, 0.6] in practice

---

### 9. Ranking Function

**Formula**: ranked_journals = argsort(S_1, S_2, ..., S_N) descending

Where S_j = S_combined for journal j

**Top-K Selection**: recommendations = ranked_journals[1:K]

Default: K = 10

---

## API Documentation

### Base URL
```
http://localhost:8000/api
```

### Authentication
Currently no authentication required (development mode).

---

### Endpoint 1: `/api/recommend`

**Method**: `POST`

**Description**: Get basic journal recommendations for an abstract.

**Request Body**:
```json
{
    "abstract": "Machine learning models for natural language processing...",
    "top_k": 10
}
```

**Parameters**:
- `abstract` (string, required): Research paper abstract
- `top_k` (integer, optional, default=10): Number of recommendations to return

**Response** (200 OK):
```json
[
    {
        "journal_name": "Journal of Machine Learning Research",
        "display_name": "Journal of Machine Learning Research",
        "similarity_combined": 0.2839,
        "is_open_access": true,
        "publisher": "JMLR.org",
        "issn": "1532-4435",
        "eissn": null
    }
]
```

**cURL Example**:
```bash
curl -X POST http://localhost:8000/api/recommend \
  -H "Content-Type: application/json" \
  -d '{"abstract": "Machine learning models...", "top_k": 5}'
```

---

### Endpoint 2: `/api/recommend-detailed`

**Method**: `POST`

**Description**: Get detailed recommendations with full score breakdown.

**Request Body**: Same as `/api/recommend`

**Response** (200 OK):
```json
{
    "recommendations": [
        {
            "journal_name": "Journal of Machine Learning Research",
            "display_name": "Journal of Machine Learning Research",
            "similarity_combined": 0.2839,
            "similarity_tfidf": 0.1234,
            "similarity_bert": 0.4567,
            "similarity_bert_general": 0.4567,
            "similarity_title": 0.3456,
            "similarity_keyword": 0.2000,
            "impact_factor_boost": 0.0500,
            "field_matching_boost": 0.0300,
            "impact_factor": 5.0,
            "is_open_access": true,
            "publisher": "JMLR.org",
            "subjects": [
                {"display_name": "Machine Learning"},
                {"display_name": "Artificial Intelligence"}
            ]
        }
    ],
    "processing_time_ms": 145.6,
    "query_text": "Machine learning models...",
    "total_journals_searched": 1776
}
```

---

## Installation & Setup

### Prerequisites

```bash
# Python 3.10+
python --version

# pip (package manager)
pip --version
```

### Installation Steps

1. **Clone Repository**
```bash
git clone https://github.com/HarshithMandi/jourrecsystem.git
cd jourrecsystem
```

2. **Create Virtual Environment**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

3. **Install Dependencies**
```bash
pip install -r requirements.txt
```

4. **Verify Database**
```bash
# Check if database exists
ls data/journal_rec.db  # Should show 37.55 MB
```

5. **Test Installation**
```bash
# Test backend
python -c "from app.services.recommender import rank_journals; print('OK')"

# Test API
python launch_api.py

# Test dashboard
python launch_dashboard.py
```

---

## Usage Examples

### Example 1: Python Script

```python
from app.services.recommender import rank_journals

# Your abstract
abstract = """
This paper presents a novel deep learning architecture for 
sentiment analysis in social media data. We propose a hybrid 
model combining convolutional and recurrent neural networks.
"""

# Get recommendations
results = rank_journals(abstract, top_k=5)

# Print results
for i, result in enumerate(results, 1):
    print(f"{i}. {result['journal_name']}")
    print(f"   Score: {result['similarity_combined']:.4f}")
    print(f"   Open Access: {'Yes' if result['is_open_access'] else 'No'}")
```

---

### Example 2: API Request

```python
import requests

abstract = "Genome-wide association studies..."

response = requests.post(
    "http://localhost:8000/api/recommend-detailed",
    json={"abstract": abstract, "top_k": 10}
)

data = response.json()
for rec in data['recommendations'][:3]:
    print(f"Journal: {rec['journal_name']}")
    print(f"Score: {rec['similarity_combined']:.4f}")
```

---

## Performance Specifications

### Query Performance

| Metric | Value | Notes |
|--------|-------|-------|
| Average Latency | 100-200 ms | For top-10 query |
| p50 Latency | 120 ms | Median response time |
| p95 Latency | 180 ms | 95th percentile |
| p99 Latency | 250 ms | 99th percentile |

**Latency Breakdown**:
- BERT Encoding: 50 ms (33%)
- Database Query: 20 ms (13%)
- Similarity Calc: 30 ms (20%)
- Title Encoding: 40 ms (27%)
- Result Format: 10 ms (7%)

### Throughput

| Concurrent Users | Queries/Second | Response Time |
|------------------|----------------|---------------|
| 1 | 8-10 QPS | 100 ms |
| 5 | 30-35 QPS | 150 ms |
| 10 | 50-60 QPS | 200 ms |

### Memory Usage

```
Process Memory: 800 MB - 1.2 GB
├─ BERT Model: 400 MB
├─ TF-IDF Model: 100 MB
├─ Python Runtime: 200 MB
└─ Vector Cache: 100-300 MB
```

---

## Troubleshooting

### Issue 1: Vector Dimension Mismatch

**Error**: `ValueError: shapes (384,) and (674,) not aligned`

**Solution**:
```python
# Rebuild all vectors
python scripts/build_vectors_crossdisciplinary.py
```

---

### Issue 2: Slow Query Performance

**Solutions**:
1. Use GPU: Set `USE_GPU=True` in config
2. Reduce top_k: Return fewer results
3. Cache vectors: Use Redis
4. Batch queries: Process multiple abstracts together

---

### Issue 3: Database Locked

**Error**: `sqlite3.OperationalError: database is locked`

**Solution**:
```python
# Add timeout
engine = create_engine(
    "sqlite:///./data/journal_rec.db",
    connect_args={"timeout": 30}
)
```

---

## Appendix

### File Structure

```
project-1/
├── app/
│   ├── main.py                          # FastAPI app
│   ├── api/routes.py                    # API endpoints
│   ├── core/config.py                   # Configuration
│   ├── models/entities.py               # ORM models
│   └── services/recommender.py          # Core algorithm
├── data/
│   └── journal_rec.db                   # Active database (37.55 MB)
├── scripts/
│   ├── init_db.py                       # Database initialization
│   ├── build_vectors_crossdisciplinary.py
│   └── add_crossdisciplinary_journals.py
├── dashboard.py                         # Streamlit UI
├── requirements.txt                     # Dependencies
├── PROJECT_DOCUMENTATION.txt            # This file
└── CHANGELOG.md                         # Complete change history
```

---

### Configuration

**Environment Variables** (`.env` file):
```bash
DB_PATH=data/journal_rec.db
ENV=development
SQL_ECHO=false
OPENALEX_EMAIL=your.email@institution.edu
TOP_K=10
USE_GPU=false
```

---

### References

1. **BERT Paper**: Devlin et al. (2018). "BERT: Pre-training of Deep Bidirectional Transformers"
2. **Sentence Transformers**: Reimers & Gurevych (2019). "Sentence-BERT"
3. **TF-IDF**: Salton & McGill (1983). "Introduction to Modern Information Retrieval"
4. **OpenAlex**: https://openalex.org/
5. **FastAPI**: https://fastapi.tiangolo.com/

---

**Document Version**: 2.0  
**Last Updated**: November 8, 2025  
**Status**: Production Ready ✅  

---

*End of Documentation*
