
Journal Recommender Backend: Theory & Data Processing

Overview
This backend is designed to recommend academic journals for research papers based on the abstract text. It uses a hybrid machine learning approach combining classical NLP (TF-IDF) and modern deep learning (BERT embeddings) to match research abstracts to journals.


1. Data Ingestion
Source: OpenAlex API (https://openalex.org/)
Process:
    - Journals are fetched via paginated API requests.
    - Each journal's metadata (ID, name, publisher, ISSN, open access status, topics) is stored in a local SQLite database.
    - Optionally, works (papers) for each journal can be ingested for richer data.
Storage:
    - Journals and works are stored in normalized tables.
    - Topics and vectors are stored as JSON strings for flexibility.


2. Data Preprocessing & Vectorization
TF-IDF (Term Frequency-Inverse Document Frequency):
    - Each journal's scope text (name + publisher + topics) is vectorized using TF-IDF.
    - This captures keyword importance and frequency across all journals.
    - Result: Each journal gets a high-dimensional sparse vector representing its textual profile.

BERT Embeddings:
    - Each journal's scope text is also encoded using a pre-trained BERT model ("all-MiniLM-L6-v2").
    - BERT captures semantic meaning and context, going beyond simple keyword matching.
    - Result: Each journal gets a dense, fixed-size vector (384 dimensions) representing its semantic profile.

Storage:
    - Both TF-IDF and BERT vectors are stored in the database as JSON strings in the JournalProfile table.


3. Query Processing & Recommendation
User Query:
    - The user provides an abstract (text) of their research paper.
    - The abstract is vectorized using the same TF-IDF and BERT models as the journals.

Similarity Calculation:
    - For each journal, the system computes:
        - Cosine similarity between the query's TF-IDF vector and the journal's TF-IDF vector.
        - Cosine similarity between the query's BERT vector and the journal's BERT vector.
    - The final similarity score is a weighted average (typically 50% TF-IDF, 50% BERT).

Ranking:
    - Journals are ranked by similarity score.
    - The top-K journals (configurable, default 10) are returned as recommendations.

Audit Trail:
    - Each query and its recommendations are logged in the database for future analysis and reproducibility.


4. Theoretical Foundations
TF-IDF:
    - Measures how important a word is to a document in a corpus.
    - Good for keyword-based matching, but ignores context and meaning.

BERT (Bidirectional Encoder Representations from Transformers):
    - Deep learning model trained on large text corpora.
    - Captures context, meaning, and relationships between words.
    - Enables semantic search and matching beyond keywords.

Cosine Similarity:
    - Measures the angle between two vectors in high-dimensional space.
    - Used to quantify how similar two texts are, regardless of their length.

Hybrid Approach:
    - Combines strengths of TF-IDF (precision, keyword focus) and BERT (context, semantics).
    - More robust to variations in query phrasing and journal scope.


5. API & Usage
FastAPI Backend:
    - Exposes endpoints for recommendations, health checks, and data management.
    - /api/recommend accepts an abstract and returns ranked journals.
    - /ping for health monitoring.

Extensibility:
    - New models, features, or data sources can be added easily.
    - Designed for research reproducibility and auditability.


6. Summary
This backend provides a scalable, robust, and theoretically sound system for matching research papers to journals. By combining classical and modern NLP, it delivers recommendations that are both precise and context-aware, supporting researchers in finding the best publication venues for their work.
