# üìä Metrics and Statistics Guide

## Comprehensive Guide to Performance and Accuracy Metrics for Journal Recommendation System

---

## üéØ Executive Summary

This document outlines all metrics and statistics that can be gathered from the Journal Recommendation System to evaluate:

1. **Performance** - How fast and efficient the system is
2. **Accuracy** - How good the recommendations are
3. **System Health** - Overall system status and data quality
4. **User Behavior** - How users interact with the system

---

## üìÇ Organization

All metrics-related files are organized in the `metrics/` folder:

```
metrics/
‚îú‚îÄ‚îÄ Core Modules (Python files that collect metrics)
‚îÇ   ‚îú‚îÄ‚îÄ performance_metrics.py      # Speed & throughput tracking
‚îÇ   ‚îú‚îÄ‚îÄ accuracy_metrics.py         # Recommendation quality
‚îÇ   ‚îú‚îÄ‚îÄ system_metrics.py           # System health monitoring
‚îÇ   ‚îú‚îÄ‚îÄ user_metrics.py             # User behavior analysis
‚îÇ   ‚îî‚îÄ‚îÄ metrics_collector.py        # Central aggregation
‚îÇ
‚îî‚îÄ‚îÄ visualizations/ (Subfolder with all visualization code)
    ‚îú‚îÄ‚îÄ performance_visualizer.py   # Performance charts
    ‚îú‚îÄ‚îÄ accuracy_visualizer.py      # Accuracy plots
    ‚îú‚îÄ‚îÄ system_visualizer.py        # System dashboards
    ‚îú‚îÄ‚îÄ user_visualizer.py          # User behavior graphs
    ‚îî‚îÄ‚îÄ dashboard_generator.py      # HTML dashboard creator
```

---

## üöÄ Quick Start

### Generate Everything at Once

```bash
python metrics/example_generate_dashboard.py
```

This single command will:
- ‚úÖ Collect all metrics
- ‚úÖ Generate all visualizations
- ‚úÖ Create an interactive HTML dashboard
- ‚úÖ Save everything to `metrics/output/`

### View Individual Metrics

```bash
# Collect metrics only (JSON + text report)
python metrics/example_collect_metrics.py

# Generate visualizations only (PNG files)
python metrics/example_generate_visualizations.py
```

---

## üìä 1. PERFORMANCE METRICS

### What They Measure
How fast your system processes requests and serves recommendations.

### Key Metrics

#### A. Response Time Statistics
- **Mean Response Time**: Average time to process a request
- **Median Response Time**: Middle value (50th percentile)
- **P95 Response Time**: 95% of requests are faster than this
- **P99 Response Time**: 99% of requests are faster than this
- **Min/Max Response Time**: Fastest and slowest requests
- **Standard Deviation**: Consistency of response times

**Why it matters**: Users expect fast responses. High P95/P99 indicates some users get slow responses.

#### B. Throughput Metrics
- **Queries Per Second (QPS)**: How many queries handled per second
- **Queries Per Minute**: How many queries handled per minute
- **Queries Per Hour**: How many queries handled per hour
- **Peak Throughput**: Maximum queries handled in a time period

**Why it matters**: Indicates system capacity and scalability.

#### C. Latency Distribution
- **Histogram**: Distribution of response times across ranges (0-50ms, 50-100ms, etc.)
- **Percentile Breakdown**: P25, P50, P75, P90, P95, P99

**Why it matters**: Shows if most requests are fast or if there's high variance.

#### D. Component Breakdown
- **BERT Encoding Time**: Time spent generating BERT embeddings
- **TF-IDF Calculation Time**: Time spent on TF-IDF similarity
- **Database Operations Time**: Time spent querying database
- **Ranking Time**: Time spent sorting and ranking results
- **Response Formatting Time**: Time spent preparing API response

**Why it matters**: Identifies bottlenecks in the system.

#### E. Slow Query Identification
- **Threshold-based Detection**: Queries slower than X milliseconds
- **Top N Slowest Queries**: List of slowest requests
- **Slow Query Patterns**: Common characteristics of slow queries

**Why it matters**: Helps optimize problematic queries.

### Visualizations Generated
1. **Response Time Stats Bar Chart** - Compare mean, median, P95, P99, max
2. **Response Time Distribution Histogram** - See distribution across ranges
3. **Throughput Bar Chart** - Queries per second/minute/hour
4. **Component Breakdown Pie Chart** - Time spent in each component
5. **Slow Queries Bar Chart** - Identify slowest requests

---

## üéØ 2. ACCURACY METRICS

### What They Measure
How good your recommendations are and how well they match user needs.

### Key Metrics

#### A. Similarity Score Distribution
- **Mean Similarity**: Average relevance score
- **Median Similarity**: Middle relevance score
- **Score Distribution**: How scores spread across ranges (0-0.1, 0.1-0.2, etc.)
- **High Quality Count**: Recommendations with score > 0.7
- **Medium Quality Count**: Recommendations with score 0.4-0.7
- **Low Quality Count**: Recommendations with score < 0.4

**Why it matters**: Higher scores = more relevant recommendations.

#### B. Ranking Quality
- **Top-1 Average Score**: Average similarity of #1 ranked result
- **Top-3 Average Score**: Average similarity of top 3 results
- **Top-5 Average Score**: Average similarity of top 5 results
- **Score Drop Between Ranks**: How much scores decrease from rank to rank
- **Top-1 to Top-10 Gap**: Difference between best and 10th result
- **Ranking Consistency**: How consistent the score drops are

**Why it matters**: Shows if best results are truly better than others.

#### C. Recommendation Diversity
- **Unique Journals Recommended**: How many different journals appear
- **Unique Publishers**: Number of different publishers
- **Subject Diversity**: Number of different subject areas
- **Open Access Ratio**: Percentage of OA vs traditional journals
- **Publisher Distribution**: Spread across publishers (avoid monopoly)

**Why it matters**: Diverse recommendations give users more options.

#### D. Component Contribution Analysis
- **BERT Similarity Contribution**: How much BERT impacts final score
- **TF-IDF Contribution**: How much TF-IDF impacts final score
- **Title Match Contribution**: Impact of title similarity
- **Keyword Match Contribution**: Impact of keyword overlap
- **Impact Factor Boost**: Effect of journal impact factor
- **Field Matching Boost**: Effect of subject area matching

**Why it matters**: Understand which components drive recommendations.

### Advanced Metrics (Require Additional Data)

#### E. Precision@K
- **Precision@5**: % of top 5 results that are relevant
- **Precision@10**: % of top 10 results that are relevant

*Requires: User feedback or expert labels on relevance*

#### F. NDCG (Normalized Discounted Cumulative Gain)
- Measures ranking quality with position-based discounting
- Higher NDCG = better ranking

*Requires: Relevance scores for each result*

#### G. Mean Reciprocal Rank (MRR)
- Average of 1/(rank of first relevant result)
- Higher MRR = relevant results appear earlier

*Requires: Click or selection data*

### Visualizations Generated
1. **Similarity Distribution Histogram** - See score distribution with statistics
2. **Quality Breakdown Pie Chart** - High/medium/low quality split
3. **Ranking Quality Bar Chart** - Compare top-1, top-3, top-5 scores
4. **Diversity Dashboard** - Multi-panel view of diversity metrics

---

## üñ•Ô∏è 3. SYSTEM METRICS

### What They Measure
Overall health, data quality, and system status.

### Key Metrics

#### A. Database Statistics
- **Total Journals**: Number of journals in database
- **Journals with Profiles**: Journals that have ML vectors
- **Open Access Journals**: Number of OA journals
- **Journals with Impact Factors**: Coverage of impact factor data
- **Journals with ISSN**: Coverage of ISSN data
- **Total Works**: Research papers indexed
- **Works with Abstracts**: Papers with abstract text
- **Total Queries**: Historical queries processed
- **Total Recommendations**: Historical recommendations made

**Why it matters**: More data = better recommendations.

#### B. Vector Quality Metrics
- **TF-IDF Dimensions**: Size of TF-IDF vectors
- **BERT Dimensions**: Size of BERT vectors (384 for MiniLM)
- **TF-IDF Sparsity**: % of zeros in TF-IDF vectors
- **TF-IDF Non-Zero Features**: Average non-zero features per journal
- **Vector Norms**: Average magnitude of vectors
- **Anomaly Detection**: Zero norms, unexpected dimensions

**Why it matters**: Quality vectors = accurate similarity calculations.

#### C. Data Coverage Analysis
- **Profile Coverage**: % of journals with ML profiles
- **Impact Factor Coverage**: % with impact factor data
- **Subject Coverage**: % with subject classifications
- **Vector Coverage**: % with both TF-IDF and BERT vectors
- **Overall Quality Score**: Weighted average of all coverage

**Why it matters**: Incomplete data reduces recommendation quality.

#### D. System Health Indicators
- **Health Score**: 0-100 overall system health
- **Status**: HEALTHY / WARNING / CRITICAL
- **Health Checks**: Database accessible, journals present, vectors present, recent activity, data quality
- **Issues**: Critical problems requiring attention
- **Warnings**: Non-critical concerns

**Why it matters**: Early warning system for problems.

#### E. Publisher Statistics
- **Total Publishers**: Number of unique publishers
- **Top Publishers**: Most represented publishers
- **Average Journals per Publisher**: Distribution metric
- **Single-Journal Publishers**: Publishers with only one journal

**Why it matters**: Understand data sources and potential biases.

### Visualizations Generated
1. **Database Stats Dashboard** - Multi-panel overview
2. **Vector Quality Charts** - TF-IDF and BERT metrics
3. **System Health Gauge** - Visual health score indicator
4. **Coverage Percentages** - Horizontal bar chart

---

## üë• 4. USER BEHAVIOR METRICS

### What They Measure
How users interact with the system and what they search for.

### Key Metrics

#### A. Query Patterns
- **Total Queries**: Number of queries in time window
- **Hourly Distribution**: Queries by hour of day
- **Daily Distribution**: Queries by day
- **Peak Hour**: Busiest hour
- **Query Length Statistics**: Avg, median, min, max query length
- **Queries Per Day**: Average daily query rate

**Why it matters**: Understand usage patterns and peak times.

#### B. Popular Journals
- **Top Recommended Journals**: Most frequently recommended
- **Recommendation Count**: How many times each appears
- **Average Similarity Score**: Quality of recommendations
- **Publisher Distribution**: Which publishers dominate
- **Open Access Ratio**: OA vs traditional in recommendations

**Why it matters**: Shows which journals the system favors.

#### C. Topic Trends
- **Top Keywords**: Most common words in queries
- **Top Subject Areas**: Most queried research fields
- **Keyword Frequency**: How often each keyword appears
- **Emerging Topics**: New or trending keywords
- **Subject Distribution**: Spread across disciplines

**Why it matters**: Understand research trends and user interests.

#### D. User Interaction Patterns
- **Total Sessions**: Unique user sessions
- **Queries Per Session**: Average queries per session
- **Single-Query Sessions**: % of one-and-done users
- **Multi-Query Sessions**: % of engaged users
- **Session Duration**: Time spent per session
- **Recommendations Per Query**: Average results shown

**Why it matters**: Measure user engagement and satisfaction.

#### E. Open Access Preference
- **OA Recommendation Rate**: % of OA journals recommended
- **OA Availability Rate**: % of OA journals in database
- **OA vs Traditional Ratio**: Comparison
- **User Selection Pattern**: If OA journals are preferred

**Why it matters**: Understand OA trends and preferences.

### Advanced Metrics (Require Additional Instrumentation)

#### F. Recommendation Acceptance Rate
- **Click-Through Rate (CTR)**: % of recommendations clicked
- **Position-Based CTR**: CTR by ranking position
- **Time to First Click**: How quickly users find relevant result
- **Multiple Selections**: Average journals selected per query

*Requires: Click tracking in UI*

#### G. Query Refinement Patterns
- **Refinement Rate**: % of queries that are refined
- **Refinement Strategies**: How users modify queries
- **Query Similarity**: Similarity between consecutive queries
- **Success After Refinement**: If refinement improves results

*Requires: Session-based query analysis*

### Visualizations Generated
1. **Query Patterns Timeline** - Hourly and daily distributions
2. **Popular Journals Bar Chart** - Top recommended journals
3. **Topic Trends Word Cloud** - Keyword visualization
4. **Interaction Dashboard** - Session statistics
5. **Open Access Comparison** - OA vs traditional analysis

---

## üé® Visualization Examples

### Performance Visualizations
- üìä **Bar Charts**: Response time stats, throughput metrics
- üìà **Histograms**: Latency distribution
- ü•ß **Pie Charts**: Component time breakdown
- üìâ **Line Graphs**: Throughput over time

### Accuracy Visualizations
- üìä **Histograms**: Similarity score distribution
- ü•ß **Pie Charts**: Quality breakdown (high/medium/low)
- üìä **Bar Charts**: Top-K ranking comparison
- üìã **Dashboards**: Multi-metric diversity view

### System Visualizations
- üìä **Multi-Panel Dashboards**: Database statistics
- üìà **Bar Charts**: Vector quality metrics
- üéØ **Gauge Charts**: System health score
- üìä **Horizontal Bars**: Coverage percentages

### User Visualizations
- üìà **Time Series**: Query patterns over time
- üìä **Horizontal Bars**: Popular journals ranking
- ‚òÅÔ∏è **Word Clouds**: Topic trends (keywords)
- ü•ß **Pie Charts**: Session engagement
- üìä **Comparison Charts**: OA vs traditional

---

## üîç How to Interpret Metrics

### Good Performance Indicators
- ‚úÖ P95 response time < 500ms
- ‚úÖ P99 response time < 1000ms
- ‚úÖ Throughput > 10 queries/minute
- ‚úÖ Low standard deviation in response times
- ‚úÖ No slow queries > 2000ms

### Good Accuracy Indicators
- ‚úÖ Mean similarity score > 0.6
- ‚úÖ Top-1 average score > 0.7
- ‚úÖ >50% high-quality recommendations (score > 0.7)
- ‚úÖ Consistent score drops between ranks
- ‚úÖ High diversity (many unique journals)

### Good System Health Indicators
- ‚úÖ Health score > 80
- ‚úÖ Vector coverage > 90%
- ‚úÖ No critical issues
- ‚úÖ Data quality score > 70
- ‚úÖ No vector anomalies

### Good User Behavior Indicators
- ‚úÖ Growing query volume
- ‚úÖ High multi-query session rate (>30%)
- ‚úÖ Diverse topic coverage
- ‚úÖ Balanced publisher distribution

---

## üõ†Ô∏è Implementation Guide

### Step 1: Basic Metrics Collection

```python
from metrics.metrics_collector import MetricsCollector

collector = MetricsCollector()
metrics = collector.collect_all_metrics(hours=24)
print(metrics)
```

### Step 2: Add to API (Real-time Tracking)

```python
# In app/api/routes.py
from metrics.metrics_collector import MetricsCollector
import time

metrics = MetricsCollector()

@router.post("/recommend")
def get_recommendations(request):
    start = time.time()
    
    # Your logic here
    results = rank_journals(request.abstract)
    
    # Record metrics
    duration_ms = (time.time() - start) * 1000
    metrics.record_request(duration_ms, "recommend")
    
    return results
```

### Step 3: Generate Dashboard

```python
from metrics.visualizations.dashboard_generator import DashboardGenerator

generator = DashboardGenerator()
generator.generate_full_dashboard('dashboard.html', hours=24)
```

---

## üìà Monitoring Strategy

### Daily Monitoring
- Check system health score
- Review P95 response times
- Monitor query volume
- Check for critical issues

### Weekly Analysis
- Analyze accuracy trends
- Review popular journals
- Check topic trends
- Evaluate diversity metrics

### Monthly Review
- Compare month-over-month performance
- Analyze long-term trends
- Identify optimization opportunities
- Plan improvements

---

## üö® Alert Thresholds (Recommended)

### Critical Alerts
- Health score < 50
- P99 response time > 5000ms
- Vector coverage < 50%
- No queries in 24 hours
- Database connection failure

### Warning Alerts
- Health score < 70
- P95 response time > 1000ms
- Vector coverage < 80%
- Data quality score < 60
- Anomalies detected in vectors

---

## üìö Additional Resources

- **Full Documentation**: `metrics/README.md`
- **Example Scripts**: `metrics/example_*.py`
- **API Integration**: See routes.py for tracking examples
- **Custom Metrics**: Extend existing classes in metrics/

---

## üéì Summary

You now have a comprehensive metrics system that tracks:

‚úÖ **Performance**: Speed, throughput, latency  
‚úÖ **Accuracy**: Similarity scores, ranking quality, diversity  
‚úÖ **System Health**: Database stats, vector quality, coverage  
‚úÖ **User Behavior**: Query patterns, popular topics, engagement  

All organized in `metrics/` folder with `visualizations/` subfolder for charts.

**Next Steps**:
1. Run `python metrics/example_generate_dashboard.py`
2. Open `metrics/output/dashboard.html` in browser
3. Review all metrics and visualizations
4. Integrate real-time tracking in your API
5. Set up regular monitoring schedule

---

**Document Version**: 1.0  
**Last Updated**: 2025  
**Maintainer**: Journal Recommendation System Team
